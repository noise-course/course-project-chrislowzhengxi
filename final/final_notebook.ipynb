{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6cf9b8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, I look at the [Mobile App Country of Origin](https://nprint.github.io/benchmarks/application_identification/mobile_country_of_origin.html) dataset from the nPrint project. The goal is to predict whether a mobile app was developed in the United States, China, or India based on its network traffic. The dataset has already been processed using nPrint, so the features are ready for machine learning without having to manually parse packet captures.\n",
    "\n",
    "My main goal here is to get a basic model working and see how close I can get to the leaderboard result, which reports about **96.8 balanced accuracy** using AutoGluon. I will start with a **Random Forest classifier** as a simple baseline and then examine the results using balanced accuracy and confusion matrices. Later, I might test a few other models to see if the performance changes or if Random Forest is already good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87d537",
   "metadata": {},
   "source": [
    "## Data Preprocessing (done offline)\n",
    "\n",
    "The packet capture I downloaded was in the PcapNG format, which turned out to be incompatible with the older version of the nPrint tool that I needed for feature extraction. To work around this, I converted the original file into a regular `.pcap` file using Scapy. That conversion mainly rewrites timestamps into a format the older tools can read. \n",
    "\n",
    "\n",
    "(Note: this part is a bit tricky. I used Gemini 3.0 Pro to assist with installing Conda (my Mac isn’t compatible with some nPrint dependencies) as well as converting traffic data from `.pcapng` to `.pcap`. The conversion script (`fix_pcap_safe.py`) and the previously downloaded files are located in `course-project-chrislowzhengxi/final/offline_processing`.)\n",
    "\n",
    "\n",
    "After that, I used the `nprint` command-line tool (outside of this notebook) to extract network-level features. The tool parses each packet of the PCAP file and produces a feature table in CSV form. This CSV is what the rest of the notebook loads and analyzes. I generated it once and added it directly to the project folder so the notebook does not depend on installing nPrint or running the extraction again.\n",
    "\n",
    "The command I used was essentially:\n",
    "\n",
    "```bash\n",
    "nprint -P traffic_fixed.pcap -W traffic_features.csv -4 -t -u -p 20\n",
    "```\n",
    "\n",
    "The main flags tell nPrint to include IPv4, TCP, and UDP header information, and to grab a small part of the payload. The resulting file (`traffic_features.csv`) contains the full set of features and will be used for model training and evaluation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3122502",
   "metadata": {},
   "source": [
    "## Adding Labels to the Feature Data (done offline)\n",
    "\n",
    "After running `nprint`, I ended up with a CSV containing only packet-level features. It did not include the country label. The reason is that the label was stored inside the original PcapNG file as a packet comment rather than a normal field, so `nprint` never saw it.\n",
    "\n",
    "To move forward with supervised learning, I needed a label column. I extracted the labels from the PcapNG file using `tshark`, then merged those labels with the feature rows.\n",
    "\n",
    "### Step 1. Extract labels using Tshark\n",
    "\n",
    "Each packet in the original `traffic.pcapng` included a comment such as `13682128230572000042,china`. That comment contains both an ID and the country. Using Tshark, I pulled out the `ip.src` field and the per-packet comment:\n",
    "\n",
    "```python\n",
    "# offline_processing/extract_labels_tshark.py\n",
    "\n",
    "command = [\n",
    "    \"tshark\", \"-n\", \"-r\", pcap_file,\n",
    "    \"-Y\", \"ip\",\n",
    "    \"-T\", \"fields\",\n",
    "    \"-e\", \"ip.src\",\n",
    "    \"-e\", \"frame.comment\",\n",
    "    \"-E\", \"separator=,\",\n",
    "    \"-E\", \"quote=d\"\n",
    "]\n",
    "```\n",
    "\n",
    "This produced a small CSV with the source IP and the raw comment label.\n",
    "\n",
    "(Note: Again, this part is tricky, so I asked Gemini for help. The prompt can be seen at the top of `offline_processing/extract_labels_tshark.py`.)\n",
    "\n",
    "### Step 2. Merge with feature table\n",
    "\n",
    "The next step was to join the extracted labels with the feature rows. Since both files contained the exact same number of rows and appeared in the same order, I merged them by index.\n",
    "\n",
    "I also cleaned the label string so it only keeps the country part.\n",
    "\n",
    "This step was performed during preprocessing, so it does **not** appear in this notebook. The output of that step is the file `final_dataset_with_labels.csv`, which is the dataset used for training. It is generated from the original `traffic.pcapng` file and includes the extracted country labels. Below is the script: \n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "features_df = pd.read_csv('traffic_features.csv')\n",
    "labels_df = pd.read_csv('labels_extracted.csv')\n",
    "\n",
    "# Clean the Label Column (from \"13682128230572000042,china\" to \"china\")\n",
    "def parse_label(val):\n",
    "    if isinstance(val, str) and ',' in val:\n",
    "        return val.split(',')[-1].strip() # Take the last part (the text label)\n",
    "    return 'Unknown'\n",
    "\n",
    "labels_df['clean_label'] = labels_df['raw_label'].apply(parse_label)\n",
    "\n",
    "# Assign the label to the feature set\n",
    "features_df['label'] = labels_df['clean_label']\n",
    "\n",
    "features_df.to_csv('final_dataset_with_labels.csv', index=False)\n",
    "```\n",
    "\n",
    "### Result\n",
    "\n",
    "The final dataset now includes a `label` column at the end. This is the version used for all modeling. In the rest of the notebook, I simply load the combined CSV and treat `label` as the target.\n",
    "\n",
    "That’s all that’s needed to start training models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b9fe7",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis \n",
    "Let's start with some imports: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c45397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb8fa5",
   "metadata": {},
   "source": [
    "Now, let's examine the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319ef96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (10625, 1185)\n",
      "Labels shape:   (10625, 2)\n",
      "\n",
      "Alignment Check (First 5 IPs):\n",
      "  Feature_IP   Label_IP\n",
      "0  10.11.1.3  10.11.1.3\n",
      "1    8.8.8.8    8.8.8.8\n",
      "2  10.11.1.3  10.11.1.3\n",
      "3  10.11.1.3  10.11.1.3\n",
      "4  10.11.1.3  10.11.1.3\n",
      "\n",
      "Final Class Distribution:\n",
      "label\n",
      "india    4075\n",
      "us       3500\n",
      "china    3050\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features_df = pd.read_csv('offline_processing/traffic_features.csv')\n",
    "labels_df = pd.read_csv('offline_processing/labels_extracted.csv')\n",
    "\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "print(f\"Labels shape:   {labels_df.shape}\")\n",
    "\n",
    "# 2. Sanity Check: Do the IPs match?\n",
    "# We compare the first few IPs to make sure the rows are aligned\n",
    "print(\"\\nAlignment Check (First 5 IPs):\")\n",
    "print(pd.DataFrame({\n",
    "    'Feature_IP': features_df['src_ip'].head(),\n",
    "    'Label_IP': labels_df['tshark_ip'].head()\n",
    "}))\n",
    "\n",
    "# 3. Clean the Label Column\n",
    "# The raw_label looks like \"13682128230572000042,china\"\n",
    "# We need to split it to get just \"china\"\n",
    "def parse_label(val):\n",
    "    if isinstance(val, str) and ',' in val:\n",
    "        return val.split(',')[-1].strip() # Take the last part (the text label)\n",
    "    return 'Unknown'\n",
    "\n",
    "labels_df['clean_label'] = labels_df['raw_label'].apply(parse_label)\n",
    "\n",
    "# 4. Assign the label to the feature set\n",
    "# We assign by index (position)\n",
    "features_df['label'] = labels_df['clean_label']\n",
    "\n",
    "# 5. Check Class Distribution\n",
    "print(\"\\nFinal Class Distribution:\")\n",
    "print(features_df['label'].value_counts())\n",
    "\n",
    "# Save the final dataset\n",
    "features_df.to_csv('final_dataset_with_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88254e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netml-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
