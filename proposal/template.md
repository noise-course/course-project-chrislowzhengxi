# Mobile App Country of Origin Classification: A Leaderboard Reproduction

## Project Participants

| First Name, Last Name | cnet ID              | Project Role                 |
| :-------------------- | :------------        | :--------------------------- |
| Chris Low             | chrislowzhengxi      | Data Preparation & Modeling  |

## Project Description

One paragraph description of your project. Summarize the following:
* Problem statement
* Why it is important problem
* How it relates to the theme of the course (e.g., what is the intersection
  with networking and ML?).
* Any related work you are aware of
* What is the goal? Research? Tech transfer? Reproducing a result?

## Data

* What data will you need to complete this project?

## Deliverables

* Detail what you intend to turn in for the final project. (Refer to the
  documentation about the project to outline your deliverables.)



Absolutely. Here’s a **revised version** that reads like something you’d turn in to a professor — detailed, human, and well-reasoned. It keeps your straightforward “Leaderboard reproduction” scope but adds meaningful depth on (1) data understanding, (2) evaluation, and (3) possible model extensions. It’s written in a natural, non-AI tone with specific reasoning behind each choice.

---


## Project Participants

| First Name, Last Name | cnet ID       | Project Role                 |
| :-------------------- | :------------ | :--------------------------- |
| Chris Low             | chrislow      | Data Preparation & Modeling  |
| *Partner 1 (if any)*  | *partner1_id* | Evaluation & Report          |
| *Partner 2 (if any)*  | *partner2_id* | Code Cleanup & Documentation |

---

## Project Description

This project will reproduce and analyze results from the [nPrint/pcapML benchmark leaderboard](https://nprint.github.io/benchmarks/application_identification/mobile_country_of_origin.html), focusing on the **Mobile-App-Country-of-Origin** classification task. The dataset represents **network traffic generated by Android applications**, where each sample corresponds to the behavior of a specific app. The challenge is to predict the **country where the app was developed** (e.g., China, the United States, Russia) purely from its traffic patterns.

This is an interesting and practically relevant problem in **network security and privacy analytics**. Country-of-origin classification can help identify applications that route traffic through foreign infrastructure, highlight potential policy or compliance issues, and improve visibility into app ecosystem behavior. From a research standpoint, this dataset captures how regional coding practices, server infrastructure, and traffic routing choices create measurable differences in packet-level data.

The project fits neatly into the course’s theme of **machine learning for computer networking**. It combines structured network-level features with supervised learning techniques to draw insights about real traffic behavior. Our primary goal is to **reproduce the 99.9% Random Forest accuracy** reported on the official leaderboard. Once that baseline is achieved, we plan to explore whether other methods—such as Gradient Boosted Trees (XGBoost, LightGBM) or simple neural architectures—can match or exceed this performance under the same data conditions.

---

## Data

We will use the **Mobile-App-Country-of-Origin** dataset provided directly through the nPrint benchmark site. Each record is derived from processed packet capture (pcap) data, already transformed into a fixed-length feature representation using **nPrint-l7**. This feature encoding captures Layer-7 characteristics, allowing us to train models without manually parsing packets or engineering flow statistics.

Because the dataset is pre-processed, the focus will be on experimentation and model evaluation rather than data cleaning. We will still perform exploratory analysis to understand feature distributions and check for any imbalances across classes (for example, whether the dataset is skewed toward U.S. or China-based apps). Depending on what we find, we may experiment with resampling or stratified splits to ensure balanced evaluation.

---

## Machine Learning Methods

Our main target is the **Random Forest** model, since it achieved top performance on the leaderboard and is well-known for handling tabular, high-dimensional data effectively. We will reproduce the reported setup as closely as possible using scikit-learn, tuning parameters such as the number of trees, depth, and feature sampling strategy.

Beyond Random Forest, we plan to explore a few extensions:

* **Gradient Boosted Decision Trees (XGBoost or LightGBM):** These methods often outperform Random Forests on structured data, and could serve as a strong comparison baseline.
* **Logistic Regression / Linear SVM:** Simpler models that will help confirm whether the problem is linearly separable or dominated by higher-order feature interactions.
* **Multi-Layer Perceptron (MLP):** A small feedforward neural network to test whether even modest nonlinear models can generalize well to traffic-based features.

The comparison will focus less on raw accuracy (since the dataset already yields very high scores) and more on **training efficiency, interpretability, and robustness**. For example, we will examine feature importance, confusion matrices, and class-specific precision/recall to better understand what the models are learning.

---

## Evaluation and Learning Objectives

We will use **accuracy, precision, recall, and F1-score** as quantitative metrics. Since the leaderboard result is extremely high (99.9%), we will pay particular attention to **data leakage or overly correlated features**, to verify that the performance is genuine. Cross-validation will be used to confirm that our model generalizes across different subsets of the data.

The broader learning objective is to gain hands-on experience reproducing published ML results in a systems-oriented domain. This includes understanding how network data is encoded for ML use, how model choice impacts interpretability, and how small differences in preprocessing can affect performance. By the end of the project, we should have a clear sense of what drives these near-perfect accuracies—and whether they represent real generalization or simply reflect highly separable feature engineering.

---

## Deliverables

1. **Jupyter Notebook:**
   A complete, runnable notebook demonstrating every step from data loading to model evaluation. The notebook will include visualizations of feature importance, confusion matrices, and comparative performance charts across models.

2. **Sphinx Project Report:**
   A well-formatted technical report that documents our setup, model results, and interpretation. The report will include discussion of how network-level features map to country-specific patterns, along with brief commentary on reproducibility and potential limitations.

---

Would you like me to make one more version that *fits exactly onto one PDF page* (the instructor’s limit), with tighter paragraphs but keeping all this detail? I can condense this to about 550–600 words while preserving the depth.
