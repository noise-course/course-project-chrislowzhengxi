# Mobile App Country of Origin Classification: A Leaderboard Reproduction

## Project Participants

| First Name, Last Name | cnet ID              | Project Role                 |
| :-------------------- | :------------        | :--------------------------- |
| Chris Low             | chrislowzhengxi      | Data Preparation & Modeling  |

## Project Description

This project will reproduce and analyze results from the [nPrint/pcapML benchmark leaderboard](https://nprint.github.io/benchmarks/application_identification/mobile_country_of_origin.html), focusing on the **Mobile-App-Country-of-Origin** classification task. The dataset represents **network traffic generated by Android applications**, where each sample corresponds to the behavior of a specific app. The challenge is to predict the **country where the app was developed** (e.g., China, the United States, Russia) purely from its traffic patterns.


Our main target is the **Random Forest** model, since it achieved top performance on the leaderboard and is well-known for handling tabular, high-dimensional data effectively. We will reproduce the reported setup as closely as possible using scikit-learn, tuning parameters such as the number of trees, depth, and feature sampling strategy.

Beyond Random Forest, we plan to explore a few extensions:

* **Gradient Boosted Decision Trees (XGBoost or LightGBM):** These methods often outperform Random Forests on structured data, and could serve as a strong comparison baseline.
* **Logistic Regression / Linear SVM:** Simpler models that will help confirm whether the problem is linearly separable or dominated by higher-order feature interactions.
* **Multi-Layer Perceptron (MLP):** A small feedforward neural network to test whether even modest nonlinear models can generalize well to traffic-based features.

The comparison will focus less on raw accuracy (since the dataset already yields very high scores) and more on **training efficiency and robustness**. For example, we will examine feature importance, confusion matrices, and class-specific precision/recall to better understand what the models are learning.


We will use **accuracy, precision, recall, and F1-score** as quantitative metrics. Since the leaderboard result is extremely high (99.9%), we will pay particular attention to **data leakage or overly correlated features**, to verify that the performance is genuine. Cross-validation will be used to confirm that our model generalizes across different subsets of the data. By the end of the project, we should have a clear sense of what drives these near-perfect accuracies, and whether they represent real generalization or simply reflect highly separable feature engineering.


This problem connects directly to topics in network security and privacy. Figuring out where an app was built, based only on its network traffic, can tell us a lot about how data moves around the world. Some apps might send information through overseas servers or follow routing paths that raise privacy concerns. Understanding those patterns can help people make better decisions about what they install or allow on their networks. On the technical side, this dataset shows how regional choices in coding style, infrastructure, and server design can leave small but detectable differences in the traffic an app produces.

The project fits neatly into the course’s theme of **machine learning for computer networking**. It combines structured network-level features with supervised learning techniques to draw insights about real traffic behavior. Our primary goal is to **reproduce the 96.8% Random Forest accuracy** reported on the official leaderboard. Once that baseline is achieved, we plan to explore whether other methods—such as Gradient Boosted Trees (XGBoost, LightGBM) or simple neural architectures—can match or exceed this performance under the same data conditions.


## Data

We will use the **Mobile Country of Origin** dataset provided directly through the nPrint benchmark site. Each record is derived from processed packet capture (pcap) data, already transformed into a fixed-length feature representation using **nPrint-l7**. This feature encoding captures Layer-7 characteristics, allowing us to train models without manually parsing packets or engineering flow statistics.

Because the dataset is pre-processed, the focus will be on experimentation and model evaluation rather than data cleaning. We will still perform exploratory analysis to understand feature distributions and check for any imbalances across classes (for example, whether the dataset is skewed toward U.S. or China-based apps). Depending on what we find, we may experiment with resampling or stratified splits to ensure balanced evaluation.

## Deliverables

1. **Jupyter Notebook:**
   A complete, runnable notebook demonstrating every step from data loading to model evaluation. The notebook will include visualizations of feature importance, confusion matrices, and comparative performance charts across models.

2. **Sphinx Project Report:**
   A well-formatted technical report that documents our setup, model results, and interpretation. The report will include discussion of how network-level features map to country-specific patterns, along with brief commentary on reproducibility and potential limitations.
